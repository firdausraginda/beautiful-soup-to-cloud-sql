{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"beautiful_soup_sandbox.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOTlJrAFpilxq5sF46GOCA1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"hOJ8z4kXy8A4","executionInfo":{"status":"ok","timestamp":1635158327056,"user_tz":-420,"elapsed":390,"user":{"displayName":"raginda firdaus","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09214923422429703001"}}},"source":["from bs4 import BeautifulSoup\n","import requests\n","import re"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-Jw3xzA0MOF","executionInfo":{"status":"ok","timestamp":1635155421125,"user_tz":-420,"elapsed":1430,"user":{"displayName":"raginda firdaus","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09214923422429703001"}}},"source":["html = requests.get(\"https://www.timesjobs.com/\")"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpVRScxs0iT6","executionInfo":{"status":"ok","timestamp":1635162549236,"user_tz":-420,"elapsed":414,"user":{"displayName":"raginda firdaus","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09214923422429703001"}},"outputId":"8488cf05-0bcb-4585-bb4f-3b6d312e73f2"},"source":["soup = BeautifulSoup('''\n","<html>\n","<head>\n","  <title>\n","  The Dormouse's story\n","  </title>\n","</head>\n","<body>\n","  <p id=\"title_1\">ini title 1</p>\n","  <p id=\"title_2\">ini title 2</p>\n","  <p id=\"title_3\">ini title 3</p>\n","  <p id=\"lala_yeye\">ini title 4</p>\n","  <p id=\"5_title\">ini title 5</p>\n","  <b>\n","    The Dormouse's story\n","  </b>\n","  </p>\n","  <p class=\"story\">\n","  Once upon a time there were three little sisters; and their names were\n","  <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n","    Elsie\n","  </a>\n","  ,\n","  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n","    Lacie\n","  </a>\n","  and\n","  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n","    Tillie\n","  </a>\n","  <a id=\"link4\">\n","    John\n","  </a>\n","  ; and they lived at the bottom of a well.\n","  </p>\n","  <div href=\"href_1\">\n","    ini div 1\n","  </div>\n","  <div href=\"href_2\">\n","    ini div 2\n","  </div>\n","  <div href=\"href_3\">\n","    ini div 3\n","  </div>\n","  <div href=\"lala_href_4\">\n","    ini div 4\n","  </div>\n","  <div class=\"wrapper\">\n","    <ul>\n","      <li> list pertama </li>\n","      <li> list kedua </li>\n","      <li> list ketiga </li>\n","      <li> list keempat </li>\n","    </ul>\n","  </div>\n","</body>\n","</html>\n","''', \n","'html.parser')\n","\n","a_tags = soup.find_all('a')\n","p_tags = soup.find_all('p', id=re.compile('title$'))\n","div_tags = soup.find_all('div', href=\"href_1\")\n","wrapper_double_finds = soup.find('div', class_=\"wrapper\").find_all('li')\n","wrapper_findchild = soup.find('div', class_='wrapper').findChild()\n","\n","# for wrapper_double_find in wrapper_double_finds:\n","#   print(wrapper_double_find.text.strip())\n","\n","wrapper_lis = wrapper_findchild.find_all('li')\n","for wrapper_li in wrapper_lis:\n","  print(wrapper_li.text)\n","\n","# for a_tag in a_tags:\n","#   print(a_tag)\n","#   print(a_tag.get('class'))\n","#   if a_tag.has_attr('class'):\n","#     print(a_tag)\n","\n","# for p_tag in p_tags:\n","#   print(p_tag.get_text().strip())\n","\n","# for div_tag in div_tags:\n","#   print(div_tag)"],"execution_count":208,"outputs":[{"output_type":"stream","name":"stdout","text":[" list pertama \n"," list kedua \n"," list ketiga \n"," list keempat \n"]}]}]}